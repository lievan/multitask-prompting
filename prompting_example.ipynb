{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Mq5nlxdlNu"
      },
      "source": [
        "### Define the Prompt\n",
        "\n",
        "We define a prompt that can perform two tasks given a topic and an argument <br>\n",
        "1) classify whether the argument is a fallacy, and if so what fallacy the  argument contains <br>\n",
        "2) decide whether the argument supports or refutes the topic <br>\n",
        "Prompts templates are defined by a string where ```{}``` is a placeholder for where the inputs for a sample are to be and ```<mask>``` is filled in as the predicted label. <br>\n",
        "*Multiple masked tokens in a single prompt is currently not supported*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from prompt import Prompt, RobertaPrompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "ZLIUPefZB8Bu"
      },
      "outputs": [],
      "source": [
        "templates = {\"fallacy\": \"fallacy task. Topic: {} Text: {} This contains the fallacy: <mask>\", \n",
        "             \"stance\": \"procon task. Topic: {} Text: {} Has the relation: <mask>\"}\n",
        "\n",
        "def fallacy_policy(pred):\n",
        "  fallacies = {'AppealtoEmotion', 'RedHerring', 'NoFallacy', 'IrrelevantAuthority','AdHominem','HastyGeneralization'}\n",
        "  if pred not in fallacies: return 'UNKNOWN'\n",
        "  return pred\n",
        "def stance_policy(pred):\n",
        "  if pred not in {\"support\", \"contradict\"}: return \"UNKNOWN\"\n",
        "  return pred\n",
        "\n",
        "policies = {\"fallacy\": fallacy_policy, \"stance\": stance_policy}\n",
        "\n",
        "argument_prompt = Prompt(templates, policies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GnB9BUHfT0o"
      },
      "source": [
        "RobertaPrompt uses this class to convert a sample into it's desire prompt - for example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GXLu5H6hfR85",
        "outputId": "de695e62-0c30-4248-fd80-3454598e15c5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'fallacy task. Topic: Should we allow animal testing? Text: Animal testing abuses animals and should be dis-continued This contains the fallacy: <mask>'"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "argument_prompt.test_sample([\"Should we allow animal testing?\", \"Animal testing abuses animals and should be dis-continued\"], \"fallacy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "VsjncXpYbyEN",
        "outputId": "9d0bed29-bc09-414e-eb9e-56b4df2b3e3d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'fallacy task. Topic: Should we allow animal testing? Text: Animal testing abuses animals and should be dis-continued This contains the fallacy: NoFallacy'"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "argument_prompt.train_sample([\"Should we allow animal testing?\", \"Animal testing abuses animals and should be dis-continued\"], \"NoFallacy\", \"fallacy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpl-ALGP6BVB",
        "outputId": "ddf3d963-c844-4df8-bc3f-8d3b54cc59dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Prompts ===\n",
            "Task: fallacy, Template: fallacy task. Topic: {} Text: {} This contains the fallacy: <mask>\n",
            "Task: stance, Template: procon task. Topic: {} Text: {} Has the relation: <mask>\n"
          ]
        }
      ],
      "source": [
        "print(argument_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEgFVKDueKMd"
      },
      "source": [
        "### Load Model\n",
        "We load a model we have already trained for this task. Some sample predictions are also displayed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "eZn5mZMuQQjz"
      },
      "outputs": [],
      "source": [
        "pmodel = RobertaPrompt(model='/content/drive/MyDrive/Laidlaw Research Project/models/prompt_combined', device = torch.device('cuda'), prompt = argument_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwiHWWR_6rzS",
        "outputId": "28aca836-87c1-48e8-b411-9ff231a8857a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======== Base Model ============\n",
            "/content/drive/MyDrive/Laidlaw Research Project/models/prompt_combined\n",
            "\n",
            "======== Tasks ============\n",
            "Task: fallacy, Template: fallacy task. Topic: {} Text: {} This contains the fallacy: <mask>\n",
            "Task: stance, Template: procon task. Topic: {} Text: {} Has the relation: <mask>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(pmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLkf_WLyW8zg",
        "outputId": "98dade76-810f-40b9-96e6-ac401a8c6d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fallacy: NoFallacy\n",
            "Stance: contradict\n"
          ]
        }
      ],
      "source": [
        "fallacy = pmodel.infer([\"Should we allow animal testing?\", \"Animal testing abuses animals and should be dis-continued\"], \"fallacy\")\n",
        "stance = pmodel.infer([\"Should we allow animal testing?\", \"Animal testing abuses animals and should be dis-continued\"], \"stance\")\n",
        "print(\"Fallacy: {}\\nStance: {}\".format(fallacy, stance))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MRzsviaenxX",
        "outputId": "34621a83-8c55-4444-9959-41efd146fdac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fallacy: AdHominem\n",
            "Stance: contradict\n"
          ]
        }
      ],
      "source": [
        "fallacy = pmodel.infer([\"Should we allow animal testing?\", \"Your stupid for bringing this up, animal testing is horrible\"], \"fallacy\")\n",
        "stance = pmodel.infer([\"Should we allow animal testing?\", \"Your stupid for bringing this up, animal testing is horrible\"], \"stance\")\n",
        "print(\"Fallacy: {}\\nStance: {}\".format(fallacy, stance))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58wuvAv3evt9",
        "outputId": "0c397653-4442-4791-dfce-4fd4b273d9b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fallacy: IrrelevantAuthority\n",
            "Stance: support\n"
          ]
        }
      ],
      "source": [
        "fallacy = pmodel.infer([\"Should we allow animal testing?\", \"My Dad had a dog once, he says animal testing should be allowed\"], \"fallacy\")\n",
        "stance = pmodel.infer([\"Should we allow animal testing?\", \"My Dad had a dog once, he says animal testing should be allowed\"], \"stance\")\n",
        "print(\"Fallacy: {}\\nStance: {}\".format(fallacy, stance))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l77z7efUe4xX",
        "outputId": "053712bd-5b39-438d-ba9f-df0d52fd30b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fallacy: RedHerring\n",
            "Stance: support\n"
          ]
        }
      ],
      "source": [
        "fallacy = pmodel.infer([\"Should we allow animal testing?\", \"Everyone has a favorite animal, what is yours?\"], \"fallacy\")\n",
        "stance = pmodel.infer([\"Should we allow animal testing?\", \"Everyone has a favorite animal, what is yours?\"], \"stance\")\n",
        "print(\"Fallacy: {}\\nStance: {}\".format(fallacy, stance))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfPTCfwbXdSe",
        "outputId": "e5b2c4d5-36a6-4728-a215-c20ef86f1399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "macro f1: 0.7863247863247864\n",
            "micro f1: 0.7933884297520661\n",
            "weighted f1: 0.7943516538557861\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  contradict       0.73      0.77      0.75        48\n",
            "     support       0.84      0.81      0.83        73\n",
            "\n",
            "    accuracy                           0.79       121\n",
            "   macro avg       0.78      0.79      0.79       121\n",
            "weighted avg       0.80      0.79      0.79       121\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(pmodel.test(\"/content/drive/MyDrive/Laidlaw Research Project/data/test_samples.tsv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUwprAyYiJmm"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs2FJwbmiKwM"
      },
      "outputs": [],
      "source": [
        "pmodel = RobertaPrompt(model='roberta-large', device = torch.device('cuda'), prompt = argument_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkYSvBpmnwCy"
      },
      "outputs": [],
      "source": [
        "pmodel.train(\"sample_train_set.tsv\", \"sample_val_set.tsv\", output_dir=\"sample_model\", epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7o3GJocynwZg"
      },
      "outputs": [],
      "source": [
        "pmodel.test(\"sample_test_set.tsv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
